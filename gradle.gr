Below is **copy-/-paste content** for every file you asked about.
After you drop these in the folders shown earlier you can run:

```bash
./gradlew --no-search-upward dockerfile     # generates build/Dockerfile.python.txt + .sh
./build/Dockerfile.python.sh                # builds image locally (all Artifactory urls)
```

If that succeeds, commit & push; CDX will do the remote build and publish the `:soft` and `:nonstop` tags automatically.

---

## 1 `build.gradle`  (repo root)

```groovy
plugins {
    id 'com.citadel.python-project' version '1.5.0'
    id 'com.citadel.cigdocker'      version '1.5.0'
}

python {
    srcDir = file('src/main/python')
}

cigdocker {
    createDockerImage = true

    // registry path for ALL generated tags
    imageName = 'psdata/lifecycle-retention'

    // internal mirror so no proxy needed
    baseImage = 'artifactory.citadelgroup.com/docker-remote/python:3.12-slim'

    // two immutable tags requested by the team
    extraTags = ['soft', 'nonstop']

    includeServiceRouterSidecarRunner = false     // keep image small

    // OS packages you might need (leave or delete the file)
    dockerPrependFile = file('Dockerfile.pre')

    // shrink build context
    excludePaths += ["tests", "docs", ".vscode"]
}
```

---

## 2 `settings.gradle`

```groovy
rootProject.name = 'peoplesoft_data_archive_partitions'
```

---

## 3 `requirements.txt`

```
google-cloud-bigquery==3.17.3
google-cloud-storage==2.16.0
typer==0.12.3
```

(Adjust versions to match what you actually import.)

---

## 4 `Dockerfile.pre`  (optional)

```dockerfile
# Dockerfile.pre  – runs BEFORE your code is copied
RUN apt-get update && \
    apt-get install -y --no-install-recommends gcc && \
    rm -rf /var/lib/apt/lists/*
```

Delete the file if you don’t need system libs.

---

## 5 `.gitignore`

```
# gradle
.gradle/
build/

# python
__pycache__/
*.py[cod]
# ide
.vscode/
.idea/
```

---

## 6 `src/main/python/psdata/delete_tables.py`

```python
import argparse, logging, sys
from google.cloud import storage, bigquery
from psdata.soft_archive import (
    archive_objects,          # GCS "soft"
    snapshot_then_delete      # BigQuery "soft"
)
from psdata.hard_delete import (
    delete_objects,           # GCS "hard"
    delete_year_from_table    # BigQuery "hard"
)

def build_prefix(root, module, year):
    return f"{root}/{module}/{year}/"

def list_tables_with_fy(client, project, dataset):
    qry = f"""
        SELECT table_name
        FROM  `{project}.{dataset}.__TABLES_SUMMARY__`
        WHERE partitioning_type = 'TIME'
        AND   column_partitioning_type IS NULL
    """
    return [r.table_name for r in client.query(qry).result()]

def main():
    p = argparse.ArgumentParser(description="Delete one fiscal-year slice from BQ+GCS.")
    p.add_argument("--project",  required=True)
    p.add_argument("--dataset",  required=True)
    p.add_argument("--bucket",   required=True)
    p.add_argument("--root",     default="finsup")
    p.add_argument("--module",   default="*")
    p.add_argument("--year",     type=int, required=True)
    p.add_argument("--mode",     choices=["soft", "hard"], default="soft",
                   help="soft = reversible archive, hard = permanent delete")
    g = p.add_mutually_exclusive_group()
    g.add_argument("--files-only",  action="store_true")
    g.add_argument("--tables-only", action="store_true")
    p.add_argument("--dry-run", action="store_true")
    args = p.parse_args()

    logging.basicConfig(level=logging.INFO, stream=sys.stdout)

    # ---------- FILES section ----------------------------------------------
    if not args.tables_only:
        storage_client = storage.Client(project=args.project)
        prefix = build_prefix(args.root, args.module, args.year)

        if args.dry_run:
            logging.info("[DRY-RUN] Would touch all GCS objects under %s", prefix)
        elif args.mode == "soft":
            archive_objects(storage_client, args.bucket, prefix,
                            args.year, args.module)
        else:
            delete_objects(storage_client, args.bucket, prefix,
                           args.year, args.module, args.dry_run)

    # ---------- TABLES section ---------------------------------------------
    if not args.files_only:
        bq = bigquery.Client(project=args.project)
        tables = list_tables_with_fy(bq, args.project, args.dataset)

        if args.dry_run:
            logging.info("[DRY-RUN] Would touch %d table(s): %s", len(tables), tables)
        elif args.mode == "soft":
            for tbl in tables:
                snapshot_then_delete(bq, args.project, args.dataset, tbl, args.year)
        else:
            for tbl in tables:
                delete_year_from_table(bq, args.project, args.dataset, tbl,
                                       args.year, args.dry_run)

if __name__ == "__main__":
    main()
```

---

## 7 `src/main/python/psdata/soft_archive.py`

```python
from google.cloud import storage, bigquery
import logging, datetime as dt

# ---- GCS archive ----------------------------------------------------------
def archive_objects(storage_client, bucket_name, prefix, year, module):
    dst_bucket = storage_client.bucket("peoplesoft-archive-soft")
    src_bucket = storage_client.bucket(bucket_name)

    blobs = storage_client.list_blobs(src_bucket, prefix=prefix)
    for blob in blobs:
        dst_path = f"{prefix}{blob.name}"
        logging.info("↪  %s → %s", blob.name, dst_path)
        src_bucket.copy_blob(blob, dst_bucket, dst_path)
        blob.delete()

# ---- BigQuery snapshot ----------------------------------------------------
def snapshot_then_delete(client, project, dataset, table, year):
    ts   = dt.datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    src  = f"`{project}.{dataset}.{table}`"
    snap = f"`{project}.{dataset}.__snap_{table}_{ts}`"

    logging.info("↪  snapshot %s → %s", src, snap)
    client.query(f"""
        CREATE SNAPSHOT TABLE {snap} CLONE {src}
        OPTIONS(expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 30 DAY))
    """).result()

    logging.info("✂  delete FY %s rows from %s", year, table)
    client.query(f"DELETE FROM {src} WHERE fy_partition = {year}").result()
```

*(Your current `delete_objects` & `delete_year_from_table` stay unchanged in `hard_delete.py` or wherever you keep them.)*

---

## 8 Gradle wrapper files

* **`gradlew` / `gradlew.bat`** – copy these from any working repo or from the top-level `citadel-am/citsource` root.
* **`gradle/wrapper/gradle-wrapper.properties`** — edit **only** the URL line:

```properties
distributionUrl=https://artifactory.citadelgroup.com/gradle/gradle-8.6-bin.zip
```

*(Leave the rest of the boiler-plate as it is; the `.jar` stays alongside.)*

> Your alias for `gradlew` is fine as long as it resolves to `./gradlew` inside this folder.

---

### Final sanity check

```bash
cd peoplesoft_data_archive_partitions
./gradlew --no-search-upward help          # uses internal Gradle distro
./gradlew --no-search-upward dockerfile    # generates build/…
./build/Dockerfile.python.sh               # builds local image via Artifactory
```

Push → CDX builds → tags appear in Artifactory → Batch UI registers them.
You now have a fully dockerised PeopleSoft archive cleaner with “soft” and “nonstop” modes, and no proxy headaches.
